<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <link rel="manifest" href="/images/manifest.json">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.zcwww.top","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="Pytorch">
<meta property="og:url" content="http://www.zcwww.top/2023/07/10/Pytorch/index.html">
<meta property="og:site_name" content="欢迎来到成伟的部落">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://www.zcwww.top/2023/07/10/Pytorch/04c36a9625ab880eff6ba5e2dc1a6cdb.jpeg">
<meta property="og:image" content="http://www.zcwww.top/2023/07/10/Pytorch/image-20230713110743155.png">
<meta property="og:image" content="http://www.zcwww.top/2023/07/10/Pytorch/image-20230713112510801.png">
<meta property="og:image" content="http://www.zcwww.top/2023/07/10/Pytorch/image-20230713164601557.png">
<meta property="og:image" content="http://www.zcwww.top/2023/07/10/Pytorch/image-20230713164639478.png">
<meta property="article:published_time" content="2023-07-10T07:09:04.000Z">
<meta property="article:modified_time" content="2023-07-18T03:29:37.948Z">
<meta property="article:author" content="Markcheng">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://www.zcwww.top/2023/07/10/Pytorch/04c36a9625ab880eff6ba5e2dc1a6cdb.jpeg">

<link rel="canonical" href="http://www.zcwww.top/2023/07/10/Pytorch/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Pytorch | 欢迎来到成伟的部落</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">欢迎来到成伟的部落</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">记录生活中的点点滴滴</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/zcwww6" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.zcwww.top/2023/07/10/Pytorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.jpg">
      <meta itemprop="name" content="Markcheng">
      <meta itemprop="description" content="去爱 去工作">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="欢迎来到成伟的部落">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Pytorch
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-07-10 15:09:04" itemprop="dateCreated datePublished" datetime="2023-07-10T15:09:04+08:00">2023-07-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-07-18 11:29:37" itemprop="dateModified" datetime="2023-07-18T11:29:37+08:00">2023-07-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/2023/07/10/Pytorch/04c36a9625ab880eff6ba5e2dc1a6cdb.jpeg"></p>
<span id="more"></span>



<p><strong>“DO NOT GO GENTLE IN THE DARK”</strong></p>
<p>不知不觉又踏入了<strong>人工智能</strong>的领域</p>
<p>在这里提醒自己不要忘了学习的初衷： 一切为安全</p>
<p>希望能够将学习到的人工智能方面的知识，包括深度学习、神经网络等应用到网络安全领域</p>
<h2 id="回归问题实例"><a href="#回归问题实例" class="headerlink" title="回归问题实例"></a>回归问题实例</h2><h6 id="思路梳理："><a href="#思路梳理：" class="headerlink" title="思路梳理："></a>思路梳理：</h6><ul>
<li><p>初始化学习参数：</p>
<ul>
<li><p>learning_rate = 0.0001 #学习率</p>
</li>
<li><p>initial_b = 0 #初始b值</p>
</li>
<li><p>initial_w = 0 #初始w值</p>
</li>
<li><p> num_iterations = 10000 #迭代次数</p>
</li>
<li><p>points [x, y]  #使用的点数据</p>
</li>
</ul>
</li>
<li><p>计算初始数据的损失</p>
<ul>
<li>compute_error_for_line_given_points(initial_b, initial_w …)</li>
</ul>
</li>
<li><p>梯度下降计算</p>
<ul>
<li>gradient_descent_runner(points, initial_b, initial_w, learning_rate, num_iterations)<ul>
<li>step_gradient(b, w, np.array(points), learning_rate)</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#linier aggression</span></span><br><span class="line"><span class="comment">#comupte the Loss Function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_error_for_line_given_points</span>(<span class="params">b, w, points</span>):</span></span><br><span class="line">    totalError = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(points)):</span><br><span class="line">        x = points[i, <span class="number">0</span>]</span><br><span class="line">        y = points[i ,<span class="number">1</span>]</span><br><span class="line">        totalError += (y - (w * x + b)) ** <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> totalError / <span class="built_in">float</span>(<span class="built_in">len</span>((points)))</span><br><span class="line"></span><br><span class="line"><span class="comment">#step_gradient, compute gradient steply</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step_gradient</span>(<span class="params">b_current, w_current, points, learningRate</span>):</span></span><br><span class="line">    b_gradient = <span class="number">0</span></span><br><span class="line">    w_gradient = <span class="number">0</span></span><br><span class="line">    N = <span class="built_in">float</span>(<span class="built_in">len</span>(points))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(points)):</span><br><span class="line">        x = points[i, <span class="number">0</span>]</span><br><span class="line">        y = points[i, <span class="number">1</span>]</span><br><span class="line">        b_gradient += -(<span class="number">2</span>/N) * (y - ((w_current * x) + b_current))</span><br><span class="line">        w_gradient += -(<span class="number">2</span>/N) * x * (y - ((w_current * x) + b_current))</span><br><span class="line">    new_b = b_current - (learningRate * b_gradient)</span><br><span class="line">    new_w = w_current - (learningRate * w_gradient)</span><br><span class="line">    <span class="keyword">return</span> [new_b, new_w]</span><br><span class="line"></span><br><span class="line"><span class="comment">#梯度下降循环运算</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent_runner</span>(<span class="params">points, starting_b, starting_w, learning_rate, num_iterations</span>):</span></span><br><span class="line">    b = starting_b</span><br><span class="line">    w = starting_w</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iterations):</span><br><span class="line">        b ,m = step_gradient(b, w, np.array(points), learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [b, w]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>():</span></span><br><span class="line">    points = np.genfromtxt(<span class="string">&quot;data.csv&quot;</span>, delimiter=<span class="string">&quot;,&quot;</span>)</span><br><span class="line">    learning_rate = <span class="number">0.0001</span></span><br><span class="line">    initial_b = <span class="number">0</span></span><br><span class="line">    initial_w = <span class="number">0</span></span><br><span class="line">    num_iterations = <span class="number">10000</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Starting gradient descent at b = &#123;0&#125;, w = &#123;1&#125;, error = &#123;2&#125;&quot;</span></span><br><span class="line">          .<span class="built_in">format</span>(initial_b, initial_w, compute_error_for_line_given_points(initial_b, initial_w, points))</span><br><span class="line">          )</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Running...&quot;</span>)</span><br><span class="line">    [b, w] = gradient_descent_runner(points, initial_b, initial_w, learning_rate, num_iterations)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;After &#123;0&#125; iterations b =&#123;1&#125; , w=&#123;2&#125;, error = &#123;3&#125;&quot;</span>.</span><br><span class="line">          <span class="built_in">format</span>(num_iterations, b, w, compute_error_for_line_given_points(b, w, points))</span><br><span class="line">          )</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    run()</span><br></pre></td></tr></table></figure>

<h6 id="代码备注："><a href="#代码备注：" class="headerlink" title="代码备注："></a>代码备注：</h6><ul>
<li><p>“data.csv”使用np生成的一个x,y坐标数据，生成代码如下：</p>
<ul>
<li>np.random.rand() 生成的数据服从均值为0，方差为1的正态分布</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;data.csv&quot;</span>, <span class="string">&quot;w&quot;</span>, newline=<span class="string">&quot;&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    writer = csv.writer(file)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        x = np.random.rand() * <span class="number">100</span></span><br><span class="line">        y = np.random.rand() * <span class="number">100</span></span><br><span class="line">        writer.writerow([x, y])</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><h5 id="torch-FloatTensor"><a href="#torch-FloatTensor" class="headerlink" title="torch.FloatTensor"></a>torch.FloatTensor</h5><p>用于生成数据类型为浮点的tensor，入参可以是列表，也可以是一个维度值</p>
<h5 id="torch-range-起始值，结束值，步长"><a href="#torch-range-起始值，结束值，步长" class="headerlink" title="torch.range(起始值，结束值，步长)"></a>torch.range(起始值，结束值，步长)</h5><h2 id="搭建一个简易的神经网络"><a href="#搭建一个简易的神经网络" class="headerlink" title="搭建一个简易的神经网络"></a>搭建一个简易的神经网络</h2><ul>
<li>简介：</li>
</ul>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">1</span>.设置输入节点为<span class="number">1000</span>，隐藏层的节点为<span class="number">100</span>，输出层的节点为<span class="number">10</span></span><br><span class="line"><span class="attribute">2</span>.输入<span class="number">100</span>个具有<span class="number">1000</span>个特征的数据，经过隐藏层后变成<span class="number">100</span>个具有<span class="number">10</span>个分类结果的特征，然后将得到的结果后向传播</span><br></pre></td></tr></table></figure>



<ul>
<li>代码：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">batch_n = <span class="number">100</span> <span class="comment">#一个批次输入的数据量</span></span><br><span class="line">hidden_layer = <span class="number">100</span> <span class="comment">#隐藏层的节点个数</span></span><br><span class="line">input_data = <span class="number">1000</span> <span class="comment">#每个数据的特征为1000</span></span><br><span class="line">output_data = <span class="number">10</span> <span class="comment">#输出具有是个分类结果的特征</span></span><br><span class="line"><span class="comment"># torch.set_printoptions(threshold=np.inf) #用于解决输出中省略号的问题</span></span><br><span class="line">x = torch.randn(batch_n, input_data)</span><br><span class="line">y = torch.randn(batch_n, output_data)</span><br><span class="line"></span><br><span class="line">w1 = torch.randn(input_data, hidden_layer)</span><br><span class="line">w2 = torch.randn(hidden_layer, output_data)</span><br><span class="line"></span><br><span class="line">epoch_n = <span class="number">20</span></span><br><span class="line">lr = <span class="number">1e-6</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch_n):</span><br><span class="line">    h1 = x.mm(w1) <span class="comment">#第一层权重计算</span></span><br><span class="line">    <span class="built_in">print</span>(h1.shape)</span><br><span class="line">    h1 = h1.clamp(<span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line">    y_pred = h1.mm(w2) <span class="comment">#第二层权重计算：预计输出值</span></span><br><span class="line"></span><br><span class="line">    loss = (y_pred - y).<span class="built_in">pow</span>(<span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;epoch:&#123;&#125;,loss&#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(epoch, loss))</span><br><span class="line"></span><br><span class="line">    grad_y_pred = <span class="number">2</span> * (y_pred - y) <span class="comment">#</span></span><br><span class="line">    grad_w2 = h1.t().mm(grad_y_pred)</span><br><span class="line"></span><br><span class="line">    grad_h = grad_y_pred.clone()</span><br><span class="line">    grad_h = grad_h.mm(w2.t())</span><br><span class="line">    grad_h.clamp(<span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line">    grad_w1 = x.t().mm(grad_h)</span><br><span class="line"></span><br><span class="line">    w1 = w1 - lr * grad_w1</span><br><span class="line">    w2 = w2 - lr * grad_w2</span><br></pre></td></tr></table></figure>



<h2 id="搭建一个较完整的神经网络"><a href="#搭建一个较完整的神经网络" class="headerlink" title="搭建一个较完整的神经网络"></a>搭建一个较完整的神经网络</h2><ul>
<li><p>代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">batch_n = <span class="number">100</span>  <span class="comment"># 输入的数据量</span></span><br><span class="line">input_data = <span class="number">1000</span>  <span class="comment"># 每个数据的特征数量</span></span><br><span class="line">hidden_layer = <span class="number">100</span>  <span class="comment"># 隐藏层的神经元数量</span></span><br><span class="line">output_data = <span class="number">10</span>  <span class="comment"># 输出为的特征量的个数</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">自动梯度的功能过程大致为：先通过输入的Tensor数据类型的变量在神经网络的前向传播过程中生成一张计算图,</span></span><br><span class="line"><span class="string">然后根据这个计算图和输出结果精确计算出每一个参数需要更新的梯度，并通过完成后向传播完成对参数的梯度更新。</span></span><br><span class="line"><span class="string">完成自动梯度需要用到的torch.autograd包中的Variable类对我们定义的Tensor数据类型变量进行封装，</span></span><br><span class="line"><span class="string">在封装后，计算图中的各个节点就是一个Variable对象，这样才能应用自动梯度的功能。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">#用Variable对Tensor数据类型变量进行封装的操作。</span></span><br><span class="line"><span class="string">requires_grad如果是False，表示该变量在进行自动梯度计算的过程中不会保留梯度值。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">x = Variable(torch.randn(batch_n, input_data), requires_grad=<span class="literal">False</span>)</span><br><span class="line">y = Variable(torch.randn(batch_n, output_data), requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">w1 = Variable(torch.randn(input_data, hidden_layer), requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = Variable(torch.randn(hidden_layer, output_data), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">epoch_n = <span class="number">50</span>  <span class="comment"># 迭代次数</span></span><br><span class="line">lr = <span class="number">1e-6</span>  <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch_n):</span><br><span class="line">    h1 = x.mm(w1)  <span class="comment"># 隐藏网络的净活性</span></span><br><span class="line">    <span class="built_in">print</span>(h1.shape)</span><br><span class="line">    h1 = h1.clamp(<span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line">    y_pred = h1.mm(w2)  <span class="comment"># 预计的输出值 也是隐藏网络的活性值</span></span><br><span class="line"></span><br><span class="line">    loss = (y_pred - y).<span class="built_in">pow</span>(<span class="number">2</span>).<span class="built_in">sum</span>()  <span class="comment"># 损失函数</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;epoch:&#123;&#125;,loss:&#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(epoch, loss))</span><br><span class="line"></span><br><span class="line">    loss.backward()  <span class="comment"># 后向传播</span></span><br><span class="line"></span><br><span class="line">    w1.data -= lr * w1.grad.data</span><br><span class="line">    w2.data -= lr * w2.grad.data</span><br><span class="line"></span><br><span class="line">    w1.grad.data.zero_()</span><br><span class="line">    w2.grad.data.zero_()</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="torch-nn-Sequential类"><a href="#torch-nn-Sequential类" class="headerlink" title="torch.nn.Sequential类"></a>torch.nn.Sequential类</h3><ul>
<li>备注：</li>
</ul>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">torch.nn.Sequential类</span><br><span class="line">是torch.nn中的一种序列容器</span><br><span class="line">通过在容器中嵌套各种实现神经网络模型的搭建，</span><br><span class="line">最主要的是，参数会按照我们定义好的序列自动传递下去</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">torch.nn.Sequential括号内就是我们搭建的神经网络模型的具体结构</span><br><span class="line">Linear完成从隐藏层到输出层的线性变换，再用ReLU激活函数激活</span><br><span class="line">torch.nn.Sequential类是torch.nn中的一种序列容器，通过在容器中嵌套各种实现神经网络模型的搭建，</span><br><span class="line">最主要的是，参数会按照我们定义好的序列自动传递下去。</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line"><span class="code">   torch.nn.Linear:</span></span><br><span class="line"><span class="code">       torch.nn.Linear类用于定义模型的线性层，即完成前面提到的不同的层之间的线性变换。</span></span><br><span class="line"><span class="code">        线性层接受的参数有3个：分别是输入特征数、输出特征数、是否使用偏置，默认为True</span></span><br><span class="line"><span class="code">        使用torch.nn.Linear类，会自动生成对应维度的权重参数和偏置，对于生成的权重参数和偏置，我们的模型默认使用一种比之前的简单随机方式更好的参数初始化方式。</span></span><br><span class="line"><span class="code">   torch.nn.ReLU:</span></span><br><span class="line"><span class="code">        torch.nn.ReLU属于非线性激活分类，在定义时默认不需要传入参数。</span></span><br><span class="line"><span class="code">        当然，在torch.nn包中还有许多非线性激活函数类可供选择，比如PReLU、LeaKyReLU、Tanh、Sigmoid、Softmax等</span></span><br><span class="line"><span class="code">  </span></span><br><span class="line"><span class="code">  &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">batch_n = <span class="number">100</span>  <span class="comment"># 输入的数据量</span></span><br><span class="line">input_data = <span class="number">1000</span>  <span class="comment"># 每个数据的特征数量</span></span><br><span class="line">hidden_layer = <span class="number">100</span>  <span class="comment"># 隐藏层的神经元数量</span></span><br><span class="line">output_data = <span class="number">10</span>  <span class="comment"># 输出为的特征量的个数</span></span><br><span class="line"></span><br><span class="line">x = Variable(torch.randn(batch_n, input_data), requires_grad=<span class="literal">False</span>)</span><br><span class="line">y = Variable(torch.randn(batch_n, output_data), requires_grad=<span class="literal">False</span>)</span><br><span class="line">models = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(input_data, hidden_layer),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(hidden_layer, output_data)</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="torch-nn-MSELoss类"><a href="#torch-nn-MSELoss类" class="headerlink" title="torch.nn.MSELoss类"></a>torch.nn.MSELoss类</h3><ul>
<li><p>介绍：</p>
<p>使用均方误差函数对损失值进行计算，定义类的对象时不用传入任何参数，但在使用实例时需要输入两个维度一样的参数方可进行计算</p>
</li>
<li><p>代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss_f = torch.nn.MSELoss()</span><br><span class="line">x = Variable(torch.randn(<span class="number">100</span>, <span class="number">100</span>))</span><br><span class="line">y = Variable(torch.randn(<span class="number">100</span>, <span class="number">100</span>))</span><br><span class="line">loss = loss_f(x, y)</span><br><span class="line"><span class="built_in">print</span>(loss.data)</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="使用损失函数的神经网络"><a href="#使用损失函数的神经网络" class="headerlink" title="使用损失函数的神经网络"></a>使用损失函数的神经网络</h3><ul>
<li><p>代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">loss_f = torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line">batch_n = <span class="number">100</span></span><br><span class="line">input_data = <span class="number">1000</span></span><br><span class="line">hidden_layer = <span class="number">100</span></span><br><span class="line">output_data = <span class="number">10</span></span><br><span class="line">epoch_n = <span class="number">10000</span></span><br><span class="line">lr = <span class="number">1e-6</span></span><br><span class="line"></span><br><span class="line">x = Variable(torch.randn(batch_n, input_data), requires_grad=<span class="literal">False</span>)</span><br><span class="line">y = Variable(torch.randn(batch_n, output_data), requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">models = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(input_data, hidden_layer),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(hidden_layer, output_data)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span> (epoch_n):</span><br><span class="line">    y_pred = models(x)</span><br><span class="line">    loss = loss_f(y_pred, y)</span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;epoch:&#123;&#125;,loss:&#123;:.4f&#125;,&quot;</span>.<span class="built_in">format</span>(epoch, loss.data))</span><br><span class="line">    models.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> models.parameters():</span><br><span class="line">        param.data -= param.grad.data * lr</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="torch-optim包"><a href="#torch-optim包" class="headerlink" title="torch.optim包"></a>torch.optim包</h3><blockquote>
<p>提供非常多的可实现参数自动优化的类，如SGD、AdaGrad、RMSProp、Adam等</p>
<p>使用自动优化的类实现神经网络：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">batch_n = <span class="number">100</span>  <span class="comment"># 一个批次输入数据的数量</span></span><br><span class="line">hidden_layer = <span class="number">100</span></span><br><span class="line">input_data = <span class="number">1000</span>  <span class="comment"># 每个数据的特征为1000</span></span><br><span class="line">output_data = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">x = Variable(torch.randn(batch_n, input_data), requires_grad=<span class="literal">False</span>)</span><br><span class="line">y = Variable(torch.randn(batch_n, output_data), requires_grad=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 用Variable对Tensor数据类型变量进行封装的操作。requires_grad如果是F，表示该变量在进行自动梯度计算的过程中不会保留梯度值。</span></span><br><span class="line"></span><br><span class="line">models = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(input_data, hidden_layer),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(hidden_layer, output_data)</span><br><span class="line">)</span><br><span class="line"><span class="comment"># torch.nn.Sequential括号内就是我们搭建的神经网络模型的具体结构，Linear完成从隐藏层到输出层的线性变换，再用ReLU激活函数激活</span></span><br><span class="line"><span class="comment"># torch.nn.Sequential类是torch.nn中的一种序列容器，通过在容器中嵌套各种实现神经网络模型的搭建，</span></span><br><span class="line"><span class="comment"># 最主要的是，参数会按照我们定义好的序列自动传递下去。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">epoch_n = <span class="number">10000</span></span><br><span class="line">lr = <span class="number">1e-4</span></span><br><span class="line">loss_fn = torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line">optimzer = torch.optim.Adam(models.parameters(), lr=lr)</span><br><span class="line"><span class="comment"># 使用torch.optim.Adam类作为我们模型参数的优化函数，这里输入的是：被优化的参数和学习率的初始值。</span></span><br><span class="line"><span class="comment"># 因为我们需要优化的是模型中的全部参数，所以传递的参数是models.parameters()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行模型训练的代码如下：</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch_n):</span><br><span class="line">    y_pred = models(x)</span><br><span class="line">    loss = loss_fn(y_pred, y)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch:&#123;&#125;,Loss:&#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(epoch, loss.data))</span><br><span class="line">    optimzer.zero_grad()  <span class="comment"># 将模型参数的梯度归0</span></span><br><span class="line"></span><br><span class="line">    loss.backward()</span><br><span class="line">    optimzer.step()  <span class="comment"># 使用计算得到的梯度值对各个节点的参数进行梯度更新。</span></span><br></pre></td></tr></table></figure>





<h2 id="实例：预测房价"><a href="#实例：预测房价" class="headerlink" title="实例：预测房价"></a>实例：预测房价</h2><blockquote>
<p>根据历史数据预测未来的房价，实现一个线性回归模型，并用梯度下降算法求解该模型，从而给出预测直线、<br>求解步骤：准备数据、设计模型、训练和预测</p>
</blockquote>
<h4 id="1-准备数据"><a href="#1-准备数据" class="headerlink" title="1.准备数据"></a>1.准备数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">x = torch.linspace(<span class="number">0</span>, <span class="number">100</span>, <span class="number">100</span>).<span class="built_in">type</span>(torch.FloatTensor)  <span class="comment"># time</span></span><br><span class="line">rand = torch.randn(<span class="number">100</span>) * <span class="number">10</span>  <span class="comment"># noise</span></span><br><span class="line">y = x + rand  <span class="comment"># got history price</span></span><br><span class="line"></span><br><span class="line">x_train = x[: -<span class="number">10</span>]  <span class="comment"># training data, get the data except last 10 from x</span></span><br><span class="line">x_test = x[-<span class="number">10</span>:]  <span class="comment"># test data, get the last 10 data from x</span></span><br><span class="line">y_train = y[: -<span class="number">10</span>]  <span class="comment"># train data</span></span><br><span class="line">y_test = y[-<span class="number">10</span>:]</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对训练数据点进行可视化</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  <span class="comment"># 导入画图的程序包</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.figure(figsize=(10, 8))  # 设置绘制窗口的大小为10*8 inch</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">绘制数据，由于x和y都是自动微分变量，因此需要用data获取它们包裹的tensor，并转成numpy</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># plt.plot(x_train.data.numpy(), y_train.data.numpy(), &#x27;o&#x27;)</span></span><br><span class="line"><span class="comment"># plt.xlabel(&#x27;X&#x27;)  # 添加X轴的标注</span></span><br><span class="line"><span class="comment"># plt.ylabel(&#x27;Y&#x27;)  # 添加Y轴的标注</span></span><br><span class="line"><span class="comment"># plt.show()  # 画图</span></span><br></pre></td></tr></table></figure>

<h4 id="2-设计模型"><a href="#2-设计模型" class="headerlink" title="2.设计模型"></a>2.设计模型</h4><ul>
<li>​    希望的到一条尽可能从中间穿越这些数据散点的拟合直线</li>
<li>​    损失函数</li>
<li>​    梯度下降</li>
<li>​    学习率</li>
</ul>
<h4 id="3-训练"><a href="#3-训练" class="headerlink" title="3.训练"></a>3.训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义随机的a b （线性方程的两个参数，斜率和偏置）</span></span><br><span class="line">a = torch.rand(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.rand(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置学习率</span></span><br><span class="line">learning_rate = <span class="number">0.0001</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对a和b进行迭代计算</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line"><span class="comment"># 计算在当前ab条件下的模型预测值</span></span><br><span class="line">    <span class="comment"># 将所有的训练数据带入模型ax+b,计算每个的预测值</span></span><br><span class="line">    predictions = a.expand_as(x_train) * x_train + b.expand_as(x_train)</span><br><span class="line">    loss = torch.mean((predictions - y_train)**<span class="number">2</span>)  <span class="comment"># 计算预测值和真实值的损失值</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;loss:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(loss))</span><br><span class="line">    loss.backward()  <span class="comment"># 对损失函数进行梯度反传</span></span><br><span class="line">    <span class="comment"># 利用上一步计算中得到的a的梯度信息更新a中的data的数值</span></span><br><span class="line">    a.data.add_(- learning_rate * a.grad.data)</span><br><span class="line">    <span class="comment"># 利用上一步计算中得到的b的梯度信息更新b中的data数值</span></span><br><span class="line">    b.data.add_(- learning_rate * b.grad.data)</span><br><span class="line">    <span class="comment"># 清空存储在变量a、b中的梯度信息，以免在backward的过程中反复不停地累加</span></span><br><span class="line">    <span class="comment"># 不能直接对自动微分变量进行数值更新，只能对它的data属性进行更新</span></span><br><span class="line">    a.grad.data.zero_()</span><br><span class="line">    b.grad.data.zero_()</span><br></pre></td></tr></table></figure>



<h4 id="4-画出拟合直线"><a href="#4-画出拟合直线" class="headerlink" title="4.画出拟合直线"></a>4.画出拟合直线</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x_data = x_train.data.numpy()  <span class="comment"># 将x中的数据转换成numpy数组</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line">xplot, = plt.plot(x_data, y_train.data.numpy(), <span class="string">&#x27;o&#x27;</span>)  <span class="comment"># 绘制x,y 的散点图</span></span><br><span class="line">yplot, = plt.plot(x_data, a.data.numpy() * x_data + b.data.numpy())  <span class="comment"># 绘制直线</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Time&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Price&#x27;</span>)</span><br><span class="line">str1 = <span class="built_in">str</span>(a.data.numpy()[<span class="number">0</span>]) + <span class="string">&#x27;x +&#x27;</span> + <span class="built_in">str</span>(b.data.numpy()[<span class="number">0</span>])  <span class="comment"># 写出表达式</span></span><br><span class="line">plt.legend([xplot, yplot], [<span class="string">&#x27;Data&#x27;</span>, str1])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>效果如下：</p>
<p><img src="/2023/07/10/Pytorch/image-20230713110743155.png"></p>
<h4 id="5-模型预测"><a href="#5-模型预测" class="headerlink" title="5.模型预测"></a>5.模型预测</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">predictions = a.expand_as(x_test) * x_test + b.expand_as(x_test)</span><br><span class="line"><span class="built_in">print</span>(predictions)</span><br><span class="line"></span><br><span class="line">x_data = x_train.data.numpy()</span><br><span class="line">x_pred = x_test.data.numpy()</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line">plt.plot(x_data, y_train.data.numpy(), <span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.plot(x_pred, y_test.data.numpy(), <span class="string">&#x27;s&#x27;</span>)</span><br><span class="line">x_data = np.r_[x_data, x_test.data.numpy()]</span><br><span class="line">xplot, = plt.plot(x_data, a.data.numpy() * x_data + b.data.numpy())  <span class="comment"># 绘制拟合数据</span></span><br><span class="line">yplot, = plt.plot(x_pred, a.data.numpy() * x_pred + b.data.numpy(), <span class="string">&#x27;o&#x27;</span>)  <span class="comment"># 绘制预测数据</span></span><br><span class="line">str1 = <span class="built_in">str</span>(a.data.numpy()[<span class="number">0</span>]) + <span class="string">&#x27;x +&#x27;</span> + <span class="built_in">str</span>(b.data.numpy()[<span class="number">0</span>])  <span class="comment"># 写出表达式</span></span><br><span class="line">plt.legend([xplot, yplot], [<span class="string">&#x27;Data&#x27;</span>, str1])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>效果如下：</p>
<p><img src="/2023/07/10/Pytorch/image-20230713112510801.png"></p>
<h2 id="单车预测器"><a href="#单车预测器" class="headerlink" title="单车预测器"></a>单车预测器</h2><h4 id="1-0：使用时间变量x作为自变量"><a href="#1-0：使用时间变量x作为自变量" class="headerlink" title="1.0：使用时间变量x作为自变量"></a>1.0：使用时间变量x作为自变量</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> inline</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接在Notebook中显示输出图像</span></span><br><span class="line"></span><br><span class="line">data_path = <span class="string">&#x27;hour.csv&#x27;</span>  <span class="comment"># 读取数据</span></span><br><span class="line">rides = pd.read_csv(data_path)  <span class="comment"># rides是一个dataframe对象</span></span><br><span class="line">rides.head()  <span class="comment"># 输出部分数据</span></span><br><span class="line">counts = rides[<span class="string">&#x27;cnt&#x27;</span>][:<span class="number">50</span>]  <span class="comment"># 截取数据</span></span><br><span class="line">x = np.arange(<span class="built_in">len</span>(counts))  <span class="comment"># 获取变量x</span></span><br><span class="line">y = np.array(counts)  <span class="comment"># 单车数量为y</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">7</span>))  <span class="comment"># 设定窗口大小</span></span><br><span class="line">plt.plot(x, y, <span class="string">&#x27;o-&#x27;</span>)  <span class="comment"># 绘制原始数据</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;X&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Y&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">定义变量，通过这些变量的运算让Pytorch自动生成计算图</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># x = torch.FloatTensor(np.arange(len(counts), dtype=float))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对x 进行归一化处理</span></span><br><span class="line">x = torch.FloatTensor(np.arange(<span class="built_in">len</span>(counts), dtype=<span class="built_in">float</span>))/<span class="built_in">len</span>(counts)</span><br><span class="line">y = torch.FloatTensor(np.array(counts, dtype=<span class="built_in">float</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sz = <span class="number">10</span>  <span class="comment"># 隐含神经元的数量</span></span><br><span class="line">weights = torch.randn((<span class="number">1</span>, sz), requires_grad=<span class="literal">True</span>)  <span class="comment"># 从输入层到隐含层的权重向量</span></span><br><span class="line">biases = torch.randn(sz, requires_grad=<span class="literal">True</span>)  <span class="comment"># 隐含层的偏置向量</span></span><br><span class="line">weights2 = torch.randn((sz, <span class="number">1</span>), requires_grad=<span class="literal">True</span>)  <span class="comment"># 从隐含层到输出层的权重向量</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">训练神经网络</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">learning_rate = <span class="number">0.001</span>  <span class="comment"># 学习率</span></span><br><span class="line">losses = []  <span class="comment"># 该数组用于记录每一次迭代的损失函数值</span></span><br><span class="line">x = x.view(<span class="number">50</span>, -<span class="number">1</span>)  <span class="comment"># 重新设置shape -1表示自动设置</span></span><br><span class="line">y = y.view(<span class="number">50</span>, -<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100000</span>):</span><br><span class="line">    hidden = x * weights + biases  <span class="comment"># 隐含层</span></span><br><span class="line">    <span class="comment"># 此时hidden的尺寸时50，10，即50个数据点，10个神经单元</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将sigmoid函数运用在每个隐含层的神经元上</span></span><br><span class="line">    hidden = torch.sigmoid(hidden)</span><br><span class="line">    <span class="comment"># 隐含层输出到输出层，得到最终预测结果</span></span><br><span class="line">    predictions = hidden.mm(weights2)  <span class="comment"># 此时predications的尺寸为50,1</span></span><br><span class="line">    <span class="comment"># 计算Loss</span></span><br><span class="line">    loss = torch.mean((predictions - y)**<span class="number">2</span>)</span><br><span class="line">    losses.append(loss.data.numpy())</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;loss:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(loss))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 执行梯度下降算法，反向传播误差</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    weights.data.add_(- learning_rate * weights.grad.data)</span><br><span class="line">    biases.data.add_(- learning_rate * biases.grad.data)</span><br><span class="line">    weights2.data.add_(- learning_rate * weights2.grad.data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 清空所有的梯度值</span></span><br><span class="line">    weights.grad.data.zero_()</span><br><span class="line">    biases.grad.data.zero_()</span><br><span class="line">    weights2.grad.data.zero_()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看Loss值的下降过程</span></span><br><span class="line"><span class="comment"># plt.plot(losses)</span></span><br><span class="line"><span class="comment"># plt.xlabel(&#x27;Epoch&#x27;)</span></span><br><span class="line"><span class="comment"># plt.ylabel(&#x27;Loss&#x27;)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">绘制50个数据点的预测曲线</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># x_data = x.data.numpy()</span></span><br><span class="line"><span class="comment"># plt.figure(figsize=(10, 7))</span></span><br><span class="line"><span class="comment"># xplot, = plt.plot(x_data, y.data.numpy(), &#x27;o&#x27;)  # 绘制原始数据</span></span><br><span class="line"><span class="comment"># yplot, = plt.plot(x_data, predictions.data.numpy())  # 绘制拟合曲线</span></span><br><span class="line"><span class="comment"># plt.xlabel(&#x27;X&#x27;)</span></span><br><span class="line"><span class="comment"># plt.ylabel(&#x27;Y&#x27;)</span></span><br><span class="line"><span class="comment"># plt.legend([xplot, yplot], [&#x27;Data&#x27;, &#x27;Prediction under 1000000 epochs&#x27;])</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">用训练好的模型来做预测</span></span><br><span class="line"><span class="string">:由于训练时产生了过拟合的问题，所以预测数据域真实数据的差距很大</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">counts_predict = rides[<span class="string">&#x27;cnt&#x27;</span>][<span class="number">50</span>:<span class="number">100</span>]</span><br><span class="line">x = torch.FloatTensor((np.arange(<span class="built_in">len</span>(counts_predict), dtype=<span class="built_in">float</span>) + <span class="built_in">len</span>(counts)) / <span class="built_in">len</span>(counts))  <span class="comment"># 归一化后的测试数据</span></span><br><span class="line">y = torch.FloatTensor(np.array(counts_predict, dtype=<span class="built_in">float</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用x预测y</span></span><br><span class="line">hidden = x.expand(sz, <span class="built_in">len</span>(x)).t() * weights.expand(<span class="built_in">len</span>(x), sz)  <span class="comment"># 从输入层到隐含层的计算</span></span><br><span class="line">hidden = torch.sigmoid(hidden)  <span class="comment"># 将sigmoid函数应用在每一个神经元上</span></span><br><span class="line">predictions = hidden.mm(weights2)</span><br><span class="line">loss = torch.mean((predictions - y)**<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br><span class="line"></span><br><span class="line">x_data = x.data.numpy()</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">7</span>))</span><br><span class="line">xplot, = plt.plot(x_data, y.data.numpy(), <span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">yplot, = plt.plot(x_data, predictions.data.numpy())</span><br><span class="line">plt.xlabel(<span class="string">&#x27;X&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Y&#x27;</span>)</span><br><span class="line">plt.legend([xplot, yplot], [<span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;prediction&#x27;</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<h6 id="训练效果"><a href="#训练效果" class="headerlink" title="训练效果"></a>训练效果</h6><p>数据拟合曲线：</p>
<p><img src="/2023/07/10/Pytorch/image-20230713164601557.png"></p>
<h6 id="预测效果：-由于使用的自变量x和Y之间不存在依赖关系，得到的模型完全失能"><a href="#预测效果：-由于使用的自变量x和Y之间不存在依赖关系，得到的模型完全失能" class="headerlink" title="预测效果： 由于使用的自变量x和Y之间不存在依赖关系，得到的模型完全失能"></a>预测效果： 由于使用的自变量x和Y之间不存在依赖关系，得到的模型完全失能</h6><p>预测曲线如下：</p>
<p><img src="/2023/07/10/Pytorch/image-20230713164639478.png"></p>
<h4 id="2-0-使用相关数据预测"><a href="#2-0-使用相关数据预测" class="headerlink" title="2.0 使用相关数据预测"></a>2.0 使用相关数据预测</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">单车预测器2.0!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">数据预处理</span></span><br><span class="line"><span class="string">1.对分类类型变量的onehot编码</span></span><br><span class="line"><span class="string">2.对数值类型变量的处理</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 对类型编码变量的处理</span></span><br><span class="line">data_path = <span class="string">&#x27;hour.csv&#x27;</span>  <span class="comment"># 读取数据</span></span><br><span class="line">rides = pd.read_csv(data_path)  <span class="comment"># rides是一个dataframe对象</span></span><br><span class="line">rides.head()  <span class="comment"># 输出部分数据</span></span><br><span class="line">counts = rides[<span class="string">&#x27;cnt&#x27;</span>][:<span class="number">50</span>]  <span class="comment"># 截取数据</span></span><br><span class="line">dummy_fields = [<span class="string">&#x27;season&#x27;</span>, <span class="string">&#x27;weathersit&#x27;</span>, <span class="string">&#x27;mnth&#x27;</span>, <span class="string">&#x27;hr&#x27;</span>, <span class="string">&#x27;weekday&#x27;</span>]  <span class="comment"># 所有类型变量的名称</span></span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> dummy_fields:</span><br><span class="line">    <span class="comment"># 取出所有类型变量，将它们转换为独热编码</span></span><br><span class="line">    dummies = pd.get_dummies(rides[each], prefix=each, drop_first=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># 将新的独热编码与原有所有变量合并</span></span><br><span class="line">    rides = pd.concat([rides, dummies], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将原来的类型变量从数据表中删除</span></span><br><span class="line">fields_to_drop = [<span class="string">&#x27;instant&#x27;</span>, <span class="string">&#x27;dteday&#x27;</span>, <span class="string">&#x27;season&#x27;</span>, <span class="string">&#x27;weathersit&#x27;</span>, <span class="string">&#x27;weekday&#x27;</span>, <span class="string">&#x27;atemp&#x27;</span>, <span class="string">&#x27;mnth&#x27;</span>, <span class="string">&#x27;workingday&#x27;</span>,</span><br><span class="line">                  <span class="string">&#x27;hr&#x27;</span>]  <span class="comment"># 要删除的类型的名称</span></span><br><span class="line">data = rides.drop(fields_to_drop, axis=<span class="number">1</span>)  <span class="comment"># 删除</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对数值编码变量的处理  :对变量进行标准化（标准正态分布）</span></span><br><span class="line">quant_features = [<span class="string">&#x27;cnt&#x27;</span>, <span class="string">&#x27;temp&#x27;</span>, <span class="string">&#x27;hum&#x27;</span>, <span class="string">&#x27;windspeed&#x27;</span>]  <span class="comment"># 数值变量名称</span></span><br><span class="line">scaled_features = &#123;&#125;  <span class="comment"># 将每一个变量的均值和方差都存储到scaled_features变量中</span></span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> quant_features:</span><br><span class="line">    mean, std = data[each].mean(), data[each].std()</span><br><span class="line">    scaled_features[each] = [mean, std]</span><br><span class="line">    <span class="comment"># 对每个变量进行标准化</span></span><br><span class="line">    data.loc[:, each] = (data[each] - mean) / std</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集的划分</span></span><br><span class="line"></span><br><span class="line">test_data = data[-<span class="number">21</span> * <span class="number">24</span>:]</span><br><span class="line">train_data = data[: -<span class="number">21</span> * <span class="number">24</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 目标列包含的字段</span></span><br><span class="line">target_fields = [<span class="string">&#x27;cnt&#x27;</span>, <span class="string">&#x27;casual&#x27;</span>, <span class="string">&#x27;registered&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将训练集划分成特征变量和目标变量</span></span><br><span class="line">features, targets = train_data.drop(target_fields, axis=<span class="number">1</span>), train_data[target_fields]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将测试数据划分成特征变量和目标变量</span></span><br><span class="line">test_features = test_data.drop(target_fields, axis=<span class="number">1</span>)</span><br><span class="line">test_targets = test_data[target_fields]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据类型转换为numpy数组</span></span><br><span class="line">X = features.values</span><br><span class="line">Y = targets[<span class="string">&#x27;cnt&#x27;</span>].values</span><br><span class="line">X = X.astype(<span class="built_in">float</span>)</span><br><span class="line">Y = Y.astype(<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">Y = np.reshape(Y, [<span class="built_in">len</span>(Y), <span class="number">1</span>])</span><br><span class="line">losses = []</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">构建神经网络</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络架构， features.shape[1] 个输入单元， 10 个隐含单元， 1个输出单元</span></span><br><span class="line">input_size = features.shape[<span class="number">1</span>]</span><br><span class="line">hidden_size = <span class="number">10</span></span><br><span class="line">output_size = <span class="number">1</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">neu = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(input_size, hidden_size),</span><br><span class="line">    torch.nn.Sigmoid(),</span><br><span class="line">    torch.nn.Linear(hidden_size, output_size),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">cost = torch.nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(neu.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 每128个样本点划分为一批，在循环的时候一批一批地读取</span></span><br><span class="line">    batch_loss = []</span><br><span class="line">    <span class="comment"># start end 分别是提取一批数据的其实下表和终止下标</span></span><br><span class="line">    <span class="keyword">for</span> start <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(X), batch_size):</span><br><span class="line">        end = start + batch_size <span class="keyword">if</span> start + batch_size &lt; <span class="built_in">len</span>(X) <span class="keyword">else</span> <span class="built_in">len</span>(X)</span><br><span class="line">        xx = torch.tensor(X[start:end], dtype=torch.<span class="built_in">float</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">        yy = torch.tensor(Y[start:end], dtype=torch.<span class="built_in">float</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">        predict = neu(xx)</span><br><span class="line">        loss = cost(predict, yy)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        batch_loss.append(loss.data.numpy())</span><br><span class="line">    <span class="comment"># 每隔100步输出损失值</span></span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        losses.append(np.mean(batch_loss))</span><br><span class="line">        <span class="built_in">print</span>(i, np.mean(batch_loss))</span><br><span class="line"></span><br><span class="line">plt.plot(np.arange(<span class="built_in">len</span>(losses)) * <span class="number">100</span>, losses)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;MSE&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">测试神经网络</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用训练好的神经网络在测试集上进行预测</span></span><br><span class="line">targets = test_targets[<span class="string">&#x27;cnt&#x27;</span>]  <span class="comment"># 读取测试集的cnt数值</span></span><br><span class="line">targets = targets.values.reshape([<span class="built_in">len</span>(targets), <span class="number">1</span>])  <span class="comment"># 将数据转换成合适的tensor形式</span></span><br><span class="line">targets = targets.astype(<span class="built_in">float</span>)  <span class="comment"># 保证数据为实数</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor(test_features.values.astype(<span class="built_in">float</span>), dtype=torch.<span class="built_in">float</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = torch.tensor(targets.astype(<span class="built_in">float</span>), dtype=torch.<span class="built_in">float</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x[:<span class="number">10</span>])</span><br><span class="line"><span class="comment"># 用神经网络进行预测</span></span><br><span class="line">predict = neu(x)</span><br><span class="line">predict = predict.data.numpy()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>((predict * std + mean)[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将后21天的预测数据与真实数据画在一起并比较</span></span><br><span class="line"><span class="comment"># 横坐标轴是不同的日期，纵坐标轴是预测或者真实数据的值</span></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line"></span><br><span class="line">mean, std = scaled_features[<span class="string">&#x27;cnt&#x27;</span>]</span><br><span class="line">ax.plot(predict * std + mean, label=<span class="string">&#x27;Prediction&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">ax.plot(targets * std + mean, label=<span class="string">&#x27;Data&#x27;</span>, linestyle=<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">ax.legend()</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;Date-time&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;Counts&#x27;</span>)</span><br><span class="line"><span class="comment"># 对横坐标轴进行标注</span></span><br><span class="line">dates = pd.to_datetime(rides.loc[test_data.index][<span class="string">&#x27;dteday&#x27;</span>])</span><br><span class="line">dates = dates.apply(<span class="keyword">lambda</span> d: d.strftime(<span class="string">&#x27;%b %d&#x27;</span>))</span><br><span class="line">ax.set_xticks(np.arange(<span class="built_in">len</span>(dates))[<span class="number">12</span>::<span class="number">24</span>])</span><br><span class="line">_ = ax.set_xticklabels(dates[<span class="number">12</span>::<span class="number">24</span>], rotation=<span class="number">45</span>)</span><br></pre></td></tr></table></figure>



<h2 id="中文情绪分类器"><a href="#中文情绪分类器" class="headerlink" title="中文情绪分类器"></a>中文情绪分类器</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">搭建简单的文本分类器</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">数据处理</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 导入数据</span></span><br><span class="line">good_file = <span class="string">&#x27;data/good.txt&#x27;</span></span><br><span class="line">bad_file = <span class="string">&#x27;data/bad.txt&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.过滤标点符号</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter_punc</span>(<span class="params">sentence</span>):</span></span><br><span class="line">    sentence = re.sub(<span class="string">&quot;[\s+\.\!\/_,$%^*(+\&quot;\&#x27;“”《》?“]+|[+——！，。？、~@#￥%……&amp;*（）：]+&quot;</span>, <span class="string">&quot;&quot;</span>, sentence)</span><br><span class="line">    <span class="keyword">return</span> sentence</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.分词</span></span><br><span class="line"><span class="comment"># 扫描所有的文本，分词并建立词典，分出正面还是负面的评论，is_filter可以过滤标点符号</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Prepare_data</span>(<span class="params">good_file, bad_file, is_filter=<span class="literal">True</span></span>):</span></span><br><span class="line">    all_words = []  <span class="comment"># 存储所有的单词</span></span><br><span class="line">    pos_sentences = []  <span class="comment"># 存储正面评论</span></span><br><span class="line">    neg_sentences = []  <span class="comment"># 存储负面评论</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(good_file, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fr:</span><br><span class="line">        <span class="keyword">for</span> idx, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(fr):</span><br><span class="line">            <span class="keyword">if</span> is_filter:</span><br><span class="line">                <span class="comment"># 过滤标点符号</span></span><br><span class="line">                line = filter_punc(line)</span><br><span class="line">                <span class="comment"># 分词</span></span><br><span class="line">                words = jieba.lcut(line)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(words) &gt; <span class="number">0</span>:</span><br><span class="line">                all_words += words</span><br><span class="line">                pos_sentences.append(words)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;&#123;0&#125; 包含 &#123;1&#125;行，&#123;2&#125; 个单词。&#x27;</span>.<span class="built_in">format</span>(good_file, idx + <span class="number">1</span>, <span class="built_in">len</span>(all_words)))</span><br><span class="line"></span><br><span class="line">    count = <span class="built_in">len</span>(all_words)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(bad_file, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fr:</span><br><span class="line">        <span class="keyword">for</span> idx, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(fr):</span><br><span class="line">            <span class="keyword">if</span> is_filter:</span><br><span class="line">                line = filter_punc(line)</span><br><span class="line">                words = jieba.lcut(line)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(words) &gt; <span class="number">0</span>:</span><br><span class="line">                all_words += words</span><br><span class="line">                neg_sentences.append(words)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;&#123;0&#125; 包含 &#123;1&#125;行，&#123;2&#125; 个单词。&#x27;</span>.<span class="built_in">format</span>(good_file, idx + <span class="number">1</span>, <span class="built_in">len</span>(all_words)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3.建立单词表 每一项与&#123;w:[id,freq]&#125;</span></span><br><span class="line"></span><br><span class="line">    diction = &#123;&#125;</span><br><span class="line">    cnt = Counter(all_words)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> cnt.items():</span><br><span class="line">        diction[word] = [<span class="built_in">len</span>(diction), freq]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;字典大小：&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(diction)))</span><br><span class="line">    <span class="keyword">return</span> pos_sentences, neg_sentences, diction</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用Prepare_data, 完成数据处理工作</span></span><br><span class="line">pos_sentences, neg_sentences, diction = Prepare_data(good_file, bad_file, <span class="literal">True</span>)</span><br><span class="line">st = <span class="built_in">sorted</span>([(v[<span class="number">1</span>], w) <span class="keyword">for</span> w, v <span class="keyword">in</span> diction.items()])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找单词的编码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word2index</span>(<span class="params">word, diction</span>):</span></span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> diction:</span><br><span class="line">        value = diction[word][<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        value = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> value</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据编码获取单词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index2word</span>(<span class="params">index, diction</span>):</span></span><br><span class="line">    <span class="keyword">for</span> w, v <span class="keyword">in</span> diction.items():</span><br><span class="line">        <span class="keyword">if</span> v[<span class="number">0</span>] == index:</span><br><span class="line">            <span class="keyword">return</span> w</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">文本数据向量化</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入一个句子和相应的词典，得到这个句子的向量化表示</span></span><br><span class="line"><span class="comment"># 向量的尺寸为词典中单词的数量，向量中位置i的值表示第i个单词在sentence中出现的频率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentence2vec</span>(<span class="params">sentence, dictionary</span>):</span></span><br><span class="line">    vector = np.zeros(<span class="built_in">len</span>(dictionary))</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> sentence:</span><br><span class="line">        vector[l] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> * vector / <span class="built_in">len</span>(sentence)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历所有句子，将每一个单词映射成编码</span></span><br><span class="line">dataset = []  <span class="comment"># 数据集</span></span><br><span class="line">labels = []  <span class="comment"># 标签</span></span><br><span class="line">sentences = []  <span class="comment"># 原始句子</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理正面评论</span></span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> pos_sentences:</span><br><span class="line">    new_sentence = []</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> sentence:</span><br><span class="line">        <span class="keyword">if</span> l <span class="keyword">in</span> diction:</span><br><span class="line">            new_sentence.append(word2index(l, diction))</span><br><span class="line">    dataset.append(sentence2vec(new_sentence, diction))</span><br><span class="line">    labels.append(<span class="number">0</span>)  <span class="comment"># 正标签为0</span></span><br><span class="line">    sentences.append(sentence)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理负向评论</span></span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> neg_sentences:</span><br><span class="line">    new_sentence = []</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> sentence:</span><br><span class="line">        <span class="keyword">if</span> l <span class="keyword">in</span> diction:</span><br><span class="line">            new_sentence.append(word2index(l, diction))</span><br><span class="line">    dataset.append(sentence2vec(new_sentence, diction))</span><br><span class="line">    labels.append(<span class="number">1</span>)  <span class="comment"># 负标签为1</span></span><br><span class="line">    sentences.append(sentence)</span><br><span class="line"><span class="comment"># 打乱所有数据的顺序，形成数据集</span></span><br><span class="line"><span class="comment"># indices 为所有数据下标的排列</span></span><br><span class="line">indices = np.random.permutation(<span class="built_in">len</span>(dataset))</span><br><span class="line"><span class="comment"># 根据打乱的下标，重新生成数据集dataset,标签集labels, 以及对应的原始句子sentences</span></span><br><span class="line">dataset = [dataset[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices]</span><br><span class="line">labels = [labels[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices]</span><br><span class="line">sentences = [sentences[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分数据集</span></span><br><span class="line">test_size = <span class="built_in">int</span>(<span class="built_in">len</span>(dataset) // <span class="number">10</span>)</span><br><span class="line">train_data = dataset[<span class="number">2</span> * test_size:]</span><br><span class="line">train_label = labels[: test_size]</span><br><span class="line"></span><br><span class="line">valid_data = dataset[: test_size]</span><br><span class="line">valid_label = labels[: test_size]</span><br><span class="line"></span><br><span class="line">test_data = dataset[test_size: <span class="number">2</span> * test_size]</span><br><span class="line">test_label = labels[test_size: <span class="number">2</span> * test_size]</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">建立神经网络</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 一个简单的前馈神经网络，共三层</span></span><br><span class="line"><span class="comment"># 第一层为线性层， 加一个非线性relu, 第二层为线性层， 中间有10个隐含单元</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入维度是词典的大小，每一条评论的词袋模型</span></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="built_in">len</span>(diction), <span class="number">10</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">10</span>, <span class="number">2</span>),</span><br><span class="line">    nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义的计算一组数据分类准确率的函数</span></span><br><span class="line"><span class="comment"># predictions 为模型给出的预测结果，labels 为数据中的标签， 比较二者已确定整个神经网络当前的表现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rightness</span>(<span class="params">predictions, labels</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算预测错误率的函数，其中predictions是模型给出的一组预测结果，batch_size行num_classes列的矩阵，labels是数据之中的正确答案&quot;&quot;&quot;</span></span><br><span class="line">    pred = torch.<span class="built_in">max</span>(predictions.data, <span class="number">1</span>)[<span class="number">1</span>]  <span class="comment"># 对于任意一行（一个样本）的输出值的第1个维度，求最大，得到每一行的最大元素的下标</span></span><br><span class="line">    rights = pred.eq(labels.data.view_as(pred)).<span class="built_in">sum</span>()  <span class="comment"># 将下标与labels中包含的类别进行比较，并累计得到比较正确的数量</span></span><br><span class="line">    <span class="keyword">return</span> rights, <span class="built_in">len</span>(labels)  <span class="comment"># 返回正确的数量和这一次一共比较了多少元素</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">训练模型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 损失函数为交叉熵</span></span><br><span class="line">cost = torch.nn.NLLLoss()</span><br><span class="line"><span class="comment"># 优化算法为Adam，可以自动调节学习率</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">records = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 循环10个Epoch</span></span><br><span class="line">losses = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(train_data, train_label)):</span><br><span class="line">        x, y = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 需要将输入的数据进行适当的变形，主要是要多出一个batch_size的维度，也即第一个为1的维度</span></span><br><span class="line">        x = torch.tensor(x, requires_grad=<span class="literal">True</span>, dtype=torch.<span class="built_in">float</span>).view(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># x的尺寸：batch_size=1, len_dictionary</span></span><br><span class="line">        <span class="comment"># 标签也要加一层外衣以变成1*1的张量</span></span><br><span class="line">        y = torch.tensor(np.array([y]), dtype=torch.long)</span><br><span class="line">        <span class="comment"># y的尺寸：batch_size=1, 1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 清空梯度</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># 模型预测</span></span><br><span class="line">        predict = model(x)</span><br><span class="line">        <span class="comment"># 计算损失函数</span></span><br><span class="line">        loss = cost(predict, y)</span><br><span class="line">        <span class="comment"># 将损失函数数值加入到列表中</span></span><br><span class="line">        losses.append(loss.data.numpy())</span><br><span class="line">        <span class="comment"># 开始进行梯度反传</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 开始对参数进行一步优化</span></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每隔3000步，跑一下校验数据集的数据，输出临时结果</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">3000</span> == <span class="number">0</span>:</span><br><span class="line">            val_losses = []</span><br><span class="line">            rights = []</span><br><span class="line">            <span class="comment"># 在所有校验数据集上实验</span></span><br><span class="line">            <span class="keyword">for</span> j, val <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(valid_data, valid_label)):</span><br><span class="line">                x, y = val</span><br><span class="line">                x = torch.tensor(x, requires_grad=<span class="literal">True</span>, dtype=torch.<span class="built_in">float</span>).view(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">                y = torch.tensor(np.array([y]), dtype=torch.long)</span><br><span class="line">                predict = model(x)</span><br><span class="line">                <span class="comment"># 调用rightness函数计算准确度</span></span><br><span class="line">                right = rightness(predict, y)</span><br><span class="line">                rights.append(right)</span><br><span class="line">                loss = cost(predict, y)</span><br><span class="line">                val_losses.append(loss.data.numpy())</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将校验集合上面的平均准确度计算出来</span></span><br><span class="line">            right_ratio = <span class="number">1.0</span> * np.<span class="built_in">sum</span>([i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> rights]) / np.<span class="built_in">sum</span>([i[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> rights])</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;第&#123;&#125;轮，训练损失：&#123;:.2f&#125;, 校验损失：&#123;:.2f&#125;, 校验准确率: &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, np.mean(losses),</span><br><span class="line">                                                                                       np.mean(val_losses),</span><br><span class="line">                                                                                       right_ratio))</span><br><span class="line">            records.append([np.mean(losses), np.mean(val_losses), right_ratio])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><h6 id="实现手写数字识别"><a href="#实现手写数字识别" class="headerlink" title="实现手写数字识别"></a>实现手写数字识别</h6><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br></pre></td><td class="code"><pre><span class="line"># _*_ coding : utf-8_*_</span><br><span class="line"># @Time : 2023/7/10 15:39 </span><br><span class="line"># @Author : Chengwei</span><br><span class="line"># @File : minist</span><br><span class="line"># @Project : local</span><br><span class="line"></span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.optim as optim</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">import torch.utils.data</span><br><span class="line"></span><br><span class="line">import torchvision.datasets as dsets</span><br><span class="line">import torchvision.transforms as transforms</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">数据准备</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line"># 定义一些训练用的超参数</span><br><span class="line">image_size = 28  # 图像的总尺寸为28x28</span><br><span class="line">num_classes = 10  # 标签的种类数</span><br><span class="line">num_epochs = 20  # 训练的总循环周期</span><br><span class="line">batch_size = 64  # 一个批次的大小，64张图片</span><br><span class="line"></span><br><span class="line"># 加载MNIST数据，如果没有下载过，系统就会在当前路径下进件/data目录，便把文件存放在其中</span><br><span class="line"># MNIST数据是torchvision包自带的，可以直接调用</span><br><span class="line"># 当用户想调用自己的图像数据时，可以用torchvision.datasets.ImageFolder</span><br><span class="line"># 或torch.utils.data.TensorDataset来加载</span><br><span class="line"></span><br><span class="line">#加载训练集</span><br><span class="line">train_dataset = dsets.MNIST(</span><br><span class="line">    root=&#x27;./data&#x27;,  # 文件存放的路径</span><br><span class="line">    train=True,  # 提取训练集</span><br><span class="line">    #将图像转化为张量，在加载数据时，就可以对图像做预处理</span><br><span class="line">    transform=transforms.ToTensor(),</span><br><span class="line">    download=True  # 找不到文件时，自动下载</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 加载测试集</span><br><span class="line">test_dataset = dsets.MNIST(</span><br><span class="line">    root=&#x27;./data&#x27;,</span><br><span class="line">    train=False,</span><br><span class="line">    transform=transforms.ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 训练集的加载器，自动将数据切分成批，顺序随机打乱</span><br><span class="line">train_loader = torch.utils.data.DataLoader(</span><br><span class="line">    dataset=train_dataset,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=True</span><br><span class="line">)</span><br><span class="line"># 我们希望将测试数据分成两部分，一部分作为校验数据，另一部分作为测试数据</span><br><span class="line"></span><br><span class="line">#定义下标数组indices,相当于对所有test_dataset中数据的编码</span><br><span class="line">#然后，定义下标Indices_val表示校验集数据的下标，indices_test表示测试集的下标</span><br><span class="line">indices = range(len(test_dataset))</span><br><span class="line">indices_val = indices[:5000]</span><br><span class="line">indices_test = indices[5000:]</span><br><span class="line"></span><br><span class="line">#根据下标构造两个数据集的SubsetRandomSampler采样器，它会对下标进行采样</span><br><span class="line">sampler_val = torch.utils.data.sampler.SubsetRandomSampler(indices_val)</span><br><span class="line">sampler_test = torch.utils.data.sampler.SubsetRandomSampler(indices_test)</span><br><span class="line"></span><br><span class="line"># 根据两个采样器定义加载器</span><br><span class="line">validation_loader = torch.utils.data.DataLoader(</span><br><span class="line">    dataset=test_dataset,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=False,</span><br><span class="line">    sampler=sampler_val</span><br><span class="line">)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(</span><br><span class="line">    dataset=test_dataset,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=False,</span><br><span class="line">    sampler=sampler_test</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 随便从数据集中读入一张图片，并绘制出来</span><br><span class="line">idx = 100</span><br><span class="line"></span><br><span class="line"># dataset支持下标索引，其中提取出来的元素为features,target格式，即属性和标签</span><br><span class="line"># [0]表示索引features</span><br><span class="line">muteimg = train_dataset[idx][0].numpy()</span><br><span class="line">#一般的图像包含RGB三个通道，而MNIST数据集的图像都是灰度的，只有一个通道</span><br><span class="line">#因此，我们忽略通道，把图像看做一个灰度矩阵</span><br><span class="line">#用imshow画图，会将灰度矩阵自动展现为彩色，不同灰度对应不同的颜色，从黄到紫</span><br><span class="line">plt.imshow(muteimg[0,...])</span><br><span class="line">print(&#x27;标签是：&#x27;,train_dataset[idx][1])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">构建网络</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">#定义卷积神经网络：4和8是人为指定的两个卷积层的厚度（feature map的数量)</span><br><span class="line">depth = [4, 8]</span><br><span class="line">class ConvNet(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        # 构造函数 该函数在创建一个ConvNet对象时会被执行</span><br><span class="line">        # 首先调用父类响应的构造函数</span><br><span class="line">        super(ConvNet, self).__init__()</span><br><span class="line">        #其次构造ConvNet需要用到的各个神经模块</span><br><span class="line">        #定义组件并不是真正搭建组建，只是把基本建筑砖块找好</span><br><span class="line">        #定义一个卷积层，输入通道为1，输出通道为4，窗口大小为5，padding为2</span><br><span class="line">        self.conv1 = nn.Conv2d(1, 4, 5, padding=2)</span><br><span class="line">        self.pool = nn.MaxPool2d(2, 2)  # 定义一个池化层，一个窗口为2x2的池化运算</span><br><span class="line">        # 第二层卷积，输入通道为depth[0],输出通道为depth[1], 窗口为5，padding 为2</span><br><span class="line">        self.conv2 = nn.Conv2d(depth[0], depth[1], 5, padding=2)</span><br><span class="line">        # 一个线性连接层，输入尺寸为最后一次立方体的线性平铺，输出层为512个结点</span><br><span class="line">        self.fc1 = nn.Linear(image_size // 4 * image_size // 4 * depth[1], 512)</span><br><span class="line">        self.fc2 = nn.Linear(512, num_classes)  # 最后一层线性分类单元，输入为512，输出为要分类的类别数</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # 该函数完成神经网络真正的前向运算，在这里把各个组件进行实际的拼装</span><br><span class="line">        # x的尺寸：(batch_size, image_channels, image_width, image_height)</span><br><span class="line">        x = self.conv1(x)  # 第一层卷积</span><br><span class="line">        x = F.relu(x)  # 激活函数用relu,防止 过拟合</span><br><span class="line">        # x 的尺寸：(batch_size, num_filters, imge_width, image_height)</span><br><span class="line"></span><br><span class="line">        x = self.pool(x)  # 第二层池化，将图片缩小</span><br><span class="line">        #x的尺寸：(batch_size,depth[0],image_width/2,image_height/2)</span><br><span class="line"></span><br><span class="line">        x = self.conv2(x)  # 第三层卷积，窗口为5，输入输出通道分别为depth[0]=4,depth[1]=8</span><br><span class="line">        x = F.relu(x)  # 非线性函数</span><br><span class="line">        # x的尺寸：(batch_size,depth[1],image_width/2,image_height/2)</span><br><span class="line">        </span><br><span class="line">        x = self.pool(x)  # 第四层池化，将图片缩小为原来的1/4</span><br><span class="line">        #  x的尺寸：(batch_size, depth[1], image_width/4,image_height/4)</span><br><span class="line">        </span><br><span class="line">        # 将立体图的特征图tensor压成一个一维的向量</span><br><span class="line">        # view() 函数可以将一个tensor按指定方式重新排布</span><br><span class="line">        x = x.view(-1, image_size // 4 * image_size // 4 * depth[1])</span><br><span class="line">        # x的尺寸：(batch_size, depth[1] * image_width/4*image_height/4)</span><br><span class="line">        </span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        #x的尺寸：(batch_size,num_classes)</span><br><span class="line">        </span><br><span class="line">        #输出层为 log_softmax,即概率对数值log(p(x))</span><br><span class="line">        # 使用log_softmax可以是后面的交叉熵计算更快</span><br><span class="line">        x = F.log_softmax(x, dim=1)</span><br><span class="line">        return x</span><br><span class="line">    def retrieve_features(self, x):</span><br><span class="line">        #该函数用于提取卷积神经网络的特征图，返回feature_map1</span><br><span class="line">        #feature_map2为前两层卷积层的特征图</span><br><span class="line">        feature_map1 = F.relu(self.conv1(x))</span><br><span class="line">        x = self.pool(feature_map1)</span><br><span class="line">        feature_map2 = F.relu(self.conv2())</span><br><span class="line">        return feature_map1, feature_map2 </span><br></pre></td></tr></table></figure>



<p>注：由于博客篇幅过长，后续的模块可能会单独写一个博客总结。详见<strong>分类</strong>——<strong>机器学习</strong></p>

    </div>

    
    
    
        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/images/wechat_channel.jpg">
            <span class="icon">
              <i class="fab fa-weixin"></i>
            </span>

            <span class="label">WeChat</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/06/11/%E6%B5%8F%E8%A7%88%E5%99%A8%E5%AE%89%E5%85%A8%E7%90%86%E8%AE%BA/" rel="prev" title="浏览器安全理论">
      <i class="fa fa-chevron-left"></i> 浏览器安全理论
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/07/17/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="next" title="卷积神经网络">
      卷积神经网络 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%E5%AE%9E%E4%BE%8B"><span class="nav-number">1.</span> <span class="nav-text">回归问题实例</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%80%9D%E8%B7%AF%E6%A2%B3%E7%90%86%EF%BC%9A"><span class="nav-number">1.0.0.0.1.</span> <span class="nav-text">思路梳理：</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%A4%87%E6%B3%A8%EF%BC%9A"><span class="nav-number">1.0.0.0.2.</span> <span class="nav-text">代码备注：</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensor"><span class="nav-number">2.</span> <span class="nav-text">Tensor</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#torch-FloatTensor"><span class="nav-number">2.0.0.1.</span> <span class="nav-text">torch.FloatTensor</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#torch-range-%E8%B5%B7%E5%A7%8B%E5%80%BC%EF%BC%8C%E7%BB%93%E6%9D%9F%E5%80%BC%EF%BC%8C%E6%AD%A5%E9%95%BF"><span class="nav-number">2.0.0.2.</span> <span class="nav-text">torch.range(起始值，结束值，步长)</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E6%98%93%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">3.</span> <span class="nav-text">搭建一个简易的神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%BE%83%E5%AE%8C%E6%95%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.</span> <span class="nav-text">搭建一个较完整的神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#torch-nn-Sequential%E7%B1%BB"><span class="nav-number">4.1.</span> <span class="nav-text">torch.nn.Sequential类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#torch-nn-MSELoss%E7%B1%BB"><span class="nav-number">4.2.</span> <span class="nav-text">torch.nn.MSELoss类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.3.</span> <span class="nav-text">使用损失函数的神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#torch-optim%E5%8C%85"><span class="nav-number">4.4.</span> <span class="nav-text">torch.optim包</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B%EF%BC%9A%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7"><span class="nav-number">5.</span> <span class="nav-text">实例：预测房价</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE"><span class="nav-number">5.0.1.</span> <span class="nav-text">1.准备数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.0.2.</span> <span class="nav-text">2.设计模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E8%AE%AD%E7%BB%83"><span class="nav-number">5.0.3.</span> <span class="nav-text">3.训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E7%94%BB%E5%87%BA%E6%8B%9F%E5%90%88%E7%9B%B4%E7%BA%BF"><span class="nav-number">5.0.4.</span> <span class="nav-text">4.画出拟合直线</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B"><span class="nav-number">5.0.5.</span> <span class="nav-text">5.模型预测</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%95%E8%BD%A6%E9%A2%84%E6%B5%8B%E5%99%A8"><span class="nav-number">6.</span> <span class="nav-text">单车预测器</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-0%EF%BC%9A%E4%BD%BF%E7%94%A8%E6%97%B6%E9%97%B4%E5%8F%98%E9%87%8Fx%E4%BD%9C%E4%B8%BA%E8%87%AA%E5%8F%98%E9%87%8F"><span class="nav-number">6.0.1.</span> <span class="nav-text">1.0：使用时间变量x作为自变量</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%95%88%E6%9E%9C"><span class="nav-number">6.0.1.0.1.</span> <span class="nav-text">训练效果</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E6%95%88%E6%9E%9C%EF%BC%9A-%E7%94%B1%E4%BA%8E%E4%BD%BF%E7%94%A8%E7%9A%84%E8%87%AA%E5%8F%98%E9%87%8Fx%E5%92%8CY%E4%B9%8B%E9%97%B4%E4%B8%8D%E5%AD%98%E5%9C%A8%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB%EF%BC%8C%E5%BE%97%E5%88%B0%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%AE%8C%E5%85%A8%E5%A4%B1%E8%83%BD"><span class="nav-number">6.0.1.0.2.</span> <span class="nav-text">预测效果： 由于使用的自变量x和Y之间不存在依赖关系，得到的模型完全失能</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-0-%E4%BD%BF%E7%94%A8%E7%9B%B8%E5%85%B3%E6%95%B0%E6%8D%AE%E9%A2%84%E6%B5%8B"><span class="nav-number">6.0.2.</span> <span class="nav-text">2.0 使用相关数据预测</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%AD%E6%96%87%E6%83%85%E7%BB%AA%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">7.</span> <span class="nav-text">中文情绪分类器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">8.</span> <span class="nav-text">卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB"><span class="nav-number">8.0.0.0.1.</span> <span class="nav-text">实现手写数字识别</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Markcheng"
      src="/images/head.jpg">
  <p class="site-author-name" itemprop="name">Markcheng</p>
  <div class="site-description" itemprop="description">去爱 去工作</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">58</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="sidebar-button motion-element"><i class="fa fa-comment"></i>
    Chat
  </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zcwww6" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zcwww6" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1311839468@qq.com" title="E-Mail → mailto:1311839468@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">greyArea</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
